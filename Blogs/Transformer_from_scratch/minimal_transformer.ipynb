{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rw_unqn84KVk",
        "outputId": "964f35fb-097e-4841-dc68-82d767d79bce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Vocabulary size: 29\n",
            "TransformerLM(\n",
            "  (embedding): Embedding(29, 64)\n",
            "  (positional_encoding): PositionalEncoding()\n",
            "  (decoder): Decoder(\n",
            "    (layers): ModuleList(\n",
            "      (0-1): 2 x DecoderLayer(\n",
            "        (self_attn): MultiHeadAttention(\n",
            "          (w_q): Linear(in_features=64, out_features=64, bias=True)\n",
            "          (w_k): Linear(in_features=64, out_features=64, bias=True)\n",
            "          (w_v): Linear(in_features=64, out_features=64, bias=True)\n",
            "          (w_o): Linear(in_features=64, out_features=64, bias=True)\n",
            "        )\n",
            "        (cross_attn): MultiHeadAttention(\n",
            "          (w_q): Linear(in_features=64, out_features=64, bias=True)\n",
            "          (w_k): Linear(in_features=64, out_features=64, bias=True)\n",
            "          (w_v): Linear(in_features=64, out_features=64, bias=True)\n",
            "          (w_o): Linear(in_features=64, out_features=64, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
            "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
            "        )\n",
            "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "        (dropout3): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (fc_out): Linear(in_features=64, out_features=29, bias=True)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            ")\n",
            "Number of parameters: 137245\n",
            "Epoch 1/10, Batch 10/39, Loss: 3.1318\n",
            "Epoch 1/10, Batch 20/39, Loss: 2.9526\n",
            "Epoch 1/10, Batch 30/39, Loss: 2.8485\n",
            "Epoch 1/10, Average Loss: 2.7586\n",
            "Epoch 2/10, Batch 10/39, Loss: 2.2399\n",
            "Epoch 2/10, Batch 20/39, Loss: 2.2247\n",
            "Epoch 2/10, Batch 30/39, Loss: 2.1901\n",
            "Epoch 2/10, Average Loss: 2.1441\n",
            "Epoch 3/10, Batch 10/39, Loss: 1.8807\n",
            "Epoch 3/10, Batch 20/39, Loss: 1.8746\n",
            "Epoch 3/10, Batch 30/39, Loss: 1.8292\n",
            "Epoch 3/10, Average Loss: 1.8050\n",
            "Epoch 4/10, Batch 10/39, Loss: 1.5634\n",
            "Epoch 4/10, Batch 20/39, Loss: 1.5484\n",
            "Epoch 4/10, Batch 30/39, Loss: 1.5302\n",
            "Epoch 4/10, Average Loss: 1.5205\n",
            "Epoch 5/10, Batch 10/39, Loss: 1.3960\n",
            "Epoch 5/10, Batch 20/39, Loss: 1.3652\n",
            "Epoch 5/10, Batch 30/39, Loss: 1.3575\n",
            "Epoch 5/10, Average Loss: 1.3469\n",
            "Epoch 6/10, Batch 10/39, Loss: 1.2166\n",
            "Epoch 6/10, Batch 20/39, Loss: 1.2092\n",
            "Epoch 6/10, Batch 30/39, Loss: 1.2005\n",
            "Epoch 6/10, Average Loss: 1.1884\n",
            "Epoch 7/10, Batch 10/39, Loss: 1.0898\n",
            "Epoch 7/10, Batch 20/39, Loss: 1.0969\n",
            "Epoch 7/10, Batch 30/39, Loss: 1.0898\n",
            "Epoch 7/10, Average Loss: 1.0865\n",
            "Epoch 8/10, Batch 10/39, Loss: 0.9893\n",
            "Epoch 8/10, Batch 20/39, Loss: 0.9837\n",
            "Epoch 8/10, Batch 30/39, Loss: 0.9741\n",
            "Epoch 8/10, Average Loss: 0.9554\n",
            "Epoch 9/10, Batch 10/39, Loss: 0.9149\n",
            "Epoch 9/10, Batch 20/39, Loss: 0.9107\n",
            "Epoch 9/10, Batch 30/39, Loss: 0.9003\n",
            "Epoch 9/10, Average Loss: 0.8918\n",
            "Epoch 10/10, Batch 10/39, Loss: 0.8148\n",
            "Epoch 10/10, Batch 20/39, Loss: 0.8502\n",
            "Epoch 10/10, Batch 30/39, Loss: 0.8447\n",
            "Epoch 10/10, Average Loss: 0.8641\n",
            "Generated text:\n",
            "Paris isPs Tla is eara a meang models cs geraneraneraneran\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Step 1: Define the multi-head attention mechanism\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        # Linear projections for Q, K, V\n",
        "        self.w_q = nn.Linear(d_model, d_model)\n",
        "        self.w_k = nn.Linear(d_model, d_model)\n",
        "        self.w_v = nn.Linear(d_model, d_model)\n",
        "        self.w_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        # Reshape from (batch_size, seq_len, d_model) to (batch_size, num_heads, seq_len, d_k)\n",
        "        x = x.view(batch_size, -1, self.num_heads, self.d_k)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        batch_size = q.size(0)\n",
        "\n",
        "        # Linear projections and split into heads\n",
        "        q = self.split_heads(self.w_q(q), batch_size)  # (batch_size, num_heads, seq_len_q, d_k)\n",
        "        k = self.split_heads(self.w_k(k), batch_size)  # (batch_size, num_heads, seq_len_k, d_k)\n",
        "        v = self.split_heads(self.w_v(v), batch_size)  # (batch_size, num_heads, seq_len_v, d_k)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1))  # (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scores = scores / math.sqrt(self.d_k)\n",
        "\n",
        "        # Apply mask if provided\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # Apply softmax to get attention weights\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # Apply attention weights to values\n",
        "        context = torch.matmul(attn_weights, v)  # (batch_size, num_heads, seq_len_q, d_k)\n",
        "\n",
        "        # Reshape back to (batch_size, seq_len_q, d_model)\n",
        "        context = context.permute(0, 2, 1, 3).contiguous()\n",
        "        context = context.view(batch_size, -1, self.d_model)\n",
        "\n",
        "        # Final linear projection\n",
        "        output = self.w_o(context)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Step 2: Define the position-wise feed-forward network\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(F.relu(self.fc1(x)))\n",
        "\n",
        "# Step 3: Define positional encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_length=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        # Create a positional encoding matrix\n",
        "        pe = torch.zeros(max_seq_length, d_model)\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Register the positional encoding as a buffer (not a parameter)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Add positional encoding to the input\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "# Step 4: Define encoder layer\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Self-attention with residual connection and layer normalization\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout1(attn_output))\n",
        "\n",
        "        # Feed-forward with residual connection and layer normalization\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout2(ff_output))\n",
        "\n",
        "        return x\n",
        "\n",
        "# Step 5: Define decoder layer\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
        "        # Self-attention with residual connection and layer normalization\n",
        "        self_attn_output = self.self_attn(x, x, x, tgt_mask)\n",
        "        x = self.norm1(x + self.dropout1(self_attn_output))\n",
        "\n",
        "        # Cross-attention with residual connection and layer normalization\n",
        "        cross_attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
        "        x = self.norm2(x + self.dropout2(cross_attn_output))\n",
        "\n",
        "        # Feed-forward with residual connection and layer normalization\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm3(x + self.dropout3(ff_output))\n",
        "\n",
        "        return x\n",
        "\n",
        "# Step 6: Define the transformer encoder\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            EncoderLayer(d_model, num_heads, d_ff, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return x\n",
        "\n",
        "# Step 7: Define the transformer decoder\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderLayer(d_model, num_heads, d_ff, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, enc_output, src_mask, tgt_mask)\n",
        "        return x\n",
        "\n",
        "# Step 8: Define the full transformer model for language modeling (decoder-only architecture)\n",
        "class TransformerLM(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
        "        super(TransformerLM, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Token embedding\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model)\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = Decoder(d_model, num_heads, d_ff, num_layers, dropout)\n",
        "\n",
        "        # Output projection\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Create embeddings and apply positional encoding\n",
        "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
        "        x = self.positional_encoding(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Pass through decoder (using x as both input and target)\n",
        "        # This is a decoder-only architecture common in language models like GPT\n",
        "        x = self.decoder(x, x, None, mask)\n",
        "\n",
        "        # Project to vocabulary\n",
        "        output = self.fc_out(x)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Step 9: Create a causal (triangular) mask for autoregressive generation\n",
        "def create_causal_mask(seq_len):\n",
        "    \"\"\"Create a causal attention mask to prevent attending to future tokens.\"\"\"\n",
        "    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
        "    return mask == 0  # Convert to boolean mask where True values are kept\n",
        "\n",
        "# Step 10: Define a simple tokenizer for text\n",
        "class SimpleTokenizer:\n",
        "    def __init__(self, texts=None):\n",
        "        self.char_to_idx = {}\n",
        "        self.idx_to_char = {}\n",
        "\n",
        "        if texts:\n",
        "            self.fit(texts)\n",
        "\n",
        "    def fit(self, texts):\n",
        "        # Create a set of all unique characters\n",
        "        unique_chars = set()\n",
        "        for text in texts:\n",
        "            unique_chars.update(text)\n",
        "\n",
        "        # Create mapping dictionaries\n",
        "        self.char_to_idx = {char: idx for idx, char in enumerate(sorted(unique_chars))}\n",
        "        self.idx_to_char = {idx: char for char, idx in self.char_to_idx.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        return [self.char_to_idx[char] for char in text]\n",
        "\n",
        "    def decode(self, indices):\n",
        "        return ''.join([self.idx_to_char[idx] for idx in indices])\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self):\n",
        "        return len(self.char_to_idx)\n",
        "\n",
        "# Step 11: Define a dataset class\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, seq_length):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "        # Tokenize all texts and concatenate\n",
        "        self.data = []\n",
        "        for text in texts:\n",
        "            self.data.extend(tokenizer.encode(text))\n",
        "\n",
        "        # Calculate the number of sequences we can create\n",
        "        self.num_sequences = max(0, len(self.data) - seq_length)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_sequences\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get input sequence and target (next tokens)\n",
        "        input_seq = self.data[idx:idx + self.seq_length]\n",
        "        target_seq = self.data[idx + 1:idx + self.seq_length + 1]\n",
        "\n",
        "        return (\n",
        "            torch.tensor(input_seq, dtype=torch.long),\n",
        "            torch.tensor(target_seq, dtype=torch.long)\n",
        "        )\n",
        "\n",
        "# Step 12: Training function\n",
        "def train_transformer_lm(model, dataloader, num_epochs, learning_rate, device):\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Create causal mask\n",
        "            seq_len = inputs.size(1)\n",
        "            causal_mask = create_causal_mask(seq_len).to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs, causal_mask)\n",
        "\n",
        "            # Calculate loss\n",
        "            # Reshape for cross-entropy: (batch_size * seq_len, vocab_size)\n",
        "            loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if (batch_idx + 1) % 10 == 0:\n",
        "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {batch_idx+1}/{len(dataloader)}, '\n",
        "                      f'Loss: {total_loss / (batch_idx + 1):.4f}')\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Average Loss: {total_loss / len(dataloader):.4f}')\n",
        "\n",
        "    return model\n",
        "\n",
        "# Step 13: Text generation function\n",
        "def generate_text(model, tokenizer, start_text, max_length, temperature=1.0, device='cpu'):\n",
        "    model.eval()\n",
        "\n",
        "    # Encode the start text\n",
        "    input_seq = tokenizer.encode(start_text)\n",
        "    input_tensor = torch.tensor([input_seq], dtype=torch.long).to(device)\n",
        "\n",
        "    # Generate new tokens one by one\n",
        "    for _ in range(max_length):\n",
        "        # Create causal mask\n",
        "        seq_len = input_tensor.size(1)\n",
        "        causal_mask = create_causal_mask(seq_len).to(device)\n",
        "\n",
        "        # Get predictions\n",
        "        with torch.no_grad():\n",
        "            output = model(input_tensor, causal_mask)\n",
        "\n",
        "        # Get the next token prediction (last token in the sequence)\n",
        "        next_token_logits = output[0, -1, :] / temperature\n",
        "\n",
        "        # Apply softmax to get probabilities\n",
        "        probabilities = F.softmax(next_token_logits, dim=-1)\n",
        "\n",
        "        # Sample from the probability distribution\n",
        "        next_token = torch.multinomial(probabilities, 1).item()\n",
        "\n",
        "        # Append the new token to the input sequence\n",
        "        input_tensor = torch.cat([\n",
        "            input_tensor,\n",
        "            torch.tensor([[next_token]], dtype=torch.long).to(device)\n",
        "        ], dim=1)\n",
        "\n",
        "    # Decode and return the generated text\n",
        "    generated_tokens = input_tensor[0].tolist()\n",
        "    generated_text = tokenizer.decode(generated_tokens)\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Step 14: Example usage\n",
        "def main():\n",
        "    # Sample data (for demonstration)\n",
        "    texts = [\n",
        "        \"Hello, how are you doing today?\",\n",
        "        \"Transformers are powerful neural network architectures.\",\n",
        "        \"Language models can generate coherent text.\",\n",
        "        \"PyTorch is a popular deep learning framework.\"\n",
        "    ]\n",
        "\n",
        "    # Hyperparameters\n",
        "    seq_length = 20\n",
        "    batch_size = 4\n",
        "    d_model = 64\n",
        "    num_heads = 4\n",
        "    d_ff = 256\n",
        "    num_layers = 2\n",
        "    dropout = 0.1\n",
        "    num_epochs = 10\n",
        "    learning_rate = 0.001\n",
        "\n",
        "    # Determine device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Initialize tokenizer\n",
        "    tokenizer = SimpleTokenizer(texts)\n",
        "    print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
        "\n",
        "    # Create dataset and dataloader\n",
        "    dataset = TextDataset(texts, tokenizer, seq_length)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Initialize model\n",
        "    model = TransformerLM(\n",
        "        vocab_size=tokenizer.vocab_size,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        d_ff=d_ff,\n",
        "        num_layers=num_layers,\n",
        "        dropout=dropout\n",
        "    )\n",
        "\n",
        "    # Print model summary\n",
        "    print(model)\n",
        "    print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")\n",
        "\n",
        "    # Train the model\n",
        "    model = train_transformer_lm(model, dataloader, num_epochs, learning_rate, device)\n",
        "\n",
        "    # Generate text\n",
        "    start_text = \"Paris is\"\n",
        "    generated_text = generate_text(model, tokenizer, start_text, max_length=50, device=device)\n",
        "    print(f\"Generated text:\\n{generated_text}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}