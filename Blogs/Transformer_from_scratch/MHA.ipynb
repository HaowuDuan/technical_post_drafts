{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "- [1. A quick overviw of transformer](#1-A-quick-overview-of-transformer)\n",
    "- [2. Implementation of a Small Transformer in TensorFlow](#2-implementation-of-a-small-transformer-in-tensorflow)\n",
    "  - [2.1 Pre-processing of the raw data](#21-Pre--processing-of-the-raw-data)\n",
    "  - [2.2 Attention and Multi-Head Attention](#22-attention-and-multi-head-attention)\n",
    "  - [2.3 Encoder and Decoder](#23-encoder-and-decoder)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1. A birdseye of Transformer\n",
    "\n",
    "The goal of this blog post is to provide the introduction of the core ideas behind transformer without getting drown into technical details. I want to provide a skeleton implementation of transformer so the reader can see the big picture of what it does before reading the original paper \"attention is all you need\". The code is in tensorflow but it is also easy to find pytorch code if you prefer that.(At the moment of the doing this exercise, I was more familar with tensorflow). Before we dive into MHA(Multi-head-attention), we need to transform text into list of numbers that can be easily understood by neural network. These include tokenization, embedding, positional encoding. I will explain what these does but these are less important details for understanding attention machnism. Modern language model has evolved well beyound in MHA and from what I have learned, there is no strict rule that says you have to use certain architectures. This kind of flexibility was a culture shock to me as a physicist. I know there are better blog post out there but this is my first post and I need to write a shit first before I can write better stuff. \n",
    "\n",
    "If we think transformer as a blackbox that perform certain task, you don't really want to process raw data. Tokenization, embedding, positional encoding are just the pre-processing of the data so the actual blackbox can have better performance. The actual component that does the job is the encoder and decoder. Encoder is the piece that learns pattern from the data and decoder is the part that generates output. A encoder model only has the encoder, and it is suitable for task such as classification which is all about understanding the data pattern. On the other hand, a decoder model only has the decoder, and it's great for next token generation task. Sequence-to-sequence model is a combination of both of them. There are subtle difference between the decoder in decoder-model and sequence-to-sequence model, which will be breifly mentioned at the end of the post and will be addressed in details in the next post.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2. Implementation of a Small Transformer in TensorFlow\n",
    "\n",
    "We first import necessary packages. To pre-process the data, I use BERT, which is a pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer,TFBertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2.1 Pre-processing of the raw data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define the tokenizer, which turns your input strings into tensorflow tensors. All tokenization does is to map a input word/symbol to a number. Some tokenizer map words to number(one word -> one token), while some do sub-word tokenization, for example BERT. Again, how you do it is flexible as long as it works and performs well for you model.\n",
    "\n",
    "\n",
    "The maximal length I defined here is 10, and set \"return_attention_mask=True\", which means that if the input string has less token than 10, it will fill the rest spots with zeros, while if it's greater than 10, the input will be cut off at 10. This process is called padding. Padding mask is a tensor that remembers whether certain location has meaningful information or simply padded zeros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[  101  2026  3899  2003 10140   102     0     0     0     0]], shape=(1, 10), dtype=int32)\n",
      "tf.Tensor([[1 1 1 1 1 1 0 0 0 0]], shape=(1, 10), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenization_test(input_test):\n",
    "    inputs = tokenizer(input_test, \n",
    "                       return_tensors='tf',\n",
    "                       max_length=10,\n",
    "                       return_attention_mask=True,\n",
    "                       padding='max_length',\n",
    "                       truncation=True)\n",
    "    ID_test=inputs['input_ids']\n",
    "    mask=inputs[\"attention_mask\"]\n",
    "    print(ID_test)\n",
    "    print(mask)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tokenization_test(\"My dog is cute\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is embedding, which maps numbers (token_IDs) into vectors. Alternatively, you can think of token IDs are one dimensional projections of some higher dimensional vector which encodes richer information. It's just a fixed map so we call pre-trained model as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_shape=(1, 10, 768)\n"
     ]
    }
   ],
   "source": [
    "# The embedding is the process where you map the token ID at each position to a vector of size D\n",
    "# Model output is a tensor of size 1 X N_id X D\n",
    "\n",
    "Embedding_model=TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def embedding_test(input_test):\n",
    "    inputs = tokenizer(input_test, \n",
    "                       return_tensors='tf',\n",
    "                       max_length=10,\n",
    "                       return_attention_mask=True,\n",
    "                       padding='max_length',\n",
    "                       truncation=True)\n",
    "    Embedding_tmp=Embedding_model(inputs)\n",
    "    output_embedding=Embedding_tmp.last_hidden_state\n",
    "\n",
    "    print(f\"output_shape={output_embedding.shape}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    embedding_test(\"My dog is cute\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the out put that the model takes scalar into a 768 (the embedding dimension,commonly denoted as $d_{model}$ in the literature)dimensional vector. In tensorflow, there is also pre-defined embedding layer, where you can define custom embedding dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow offers embedding layer for custom embedding\n",
    "# embedding = tf.keras.layers.Embedding(input_dim=N_id, output_dim=embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for positional encoding, we simply add information about the location of certain token into the same vector space. The same word in different location now can be distinguished in a sentence. In the original paper \"attention is all you need\", the author used a fixed map. Let us first denote the embedding is the map,\n",
    "\\begin{align}\n",
    " N_\\alpha \\rightarrow A^\\mu_\\alpha \\notag\n",
    "\\end{align}\n",
    "where $N_{\\alpha}$ is the token ID at position $\\alpha$, $A^\\mu_\\alpha$ is the corresponding embeded vector with $\\mu=0,1,.....,d_{model}-1$. It is common to choose embedding_dim to be an even number. For token at position \"$\\alpha$\", we have the positional encoding as the map $\\alpha \\rightarrow P^\\mu_\\alpha$\n",
    "\\begin{equation}\n",
    "\\begin{split}\\notag\n",
    "P_\\alpha^{2i} &= \\sin\\Big(\\frac{\\alpha}{10^{4 \\times \\frac{2i}{d_{model}}}} \\Big) \\\\\n",
    "P_\\alpha^{2i+1}&= \\cos\\Big(\\frac{\\alpha}{10^{4 \\times \\frac{2i}{d_{model}}}} \\Big)\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "with $i=0,1,2,...,\\frac{d_model}{2}-1$ and $P_\\alpha^\\mu$ is another vector living in the same space, only encoding the information about the position $\\alpha$. What we feed into the transformer is $P_\\alpha^\\mu+A_\\alpha^\\mu$.\n",
    "Again, one can use fixed map or defined a position encoding layer with learnable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed positional encoding is included for demonstration purpose\n",
    "# In practice, we can use the positional encoding layer that can be optimized during training\n",
    "# Here we assumed that N_id is N_id_max\n",
    "# Tokenization will called the number N_id for padding\n",
    "# Later we will add attention mask to deal with this during calculation of attention scores\n",
    "class fixed_pos_encod_layer(tf.keras.layers.Layer):\n",
    "      def __init__(self, N_id_max, embedding_dim):\n",
    "            assert embedding_dim % 2 == 0, \"embedding_dim must be even\"\n",
    "            super(fixed_pos_encod_layer, self).__init__()\n",
    "\n",
    "            self.N_id_max=N_id_max\n",
    "            self.embedding_dim=embedding_dim\n",
    "    \n",
    "      def _pos_encod(self):\n",
    "            # position range from 0 to N_id-1\n",
    "            # i range from 0 to embedding_dim-1\n",
    "            # Due to the alternating pattern of sin and cos, by convention, embedding_dim is even\n",
    "            # Position indices\n",
    "            pos_index = tf.range(self.N_id_max, dtype=tf.float32)[:, tf.newaxis]\n",
    "            # frequency factors, note that it is more efficient to use tf.exp then tf.pow\n",
    "            omega = tf.exp(-2*tf.range(0,self.embedding_dim,2,dtype=tf.float32)/self.embedding_dim *tf.math.log(10000.0))\n",
    "            \n",
    "            angles=pos_index*omega \n",
    "\n",
    "            pos_encoding=tf.concat([tf.sin(angles),tf.cos(angles)],axis=-1)\n",
    "            \n",
    "            # add batch dimension\n",
    "            # prepare for broadcasting\n",
    "            return tf.expand_dims(pos_encoding,0)\n",
    "      \n",
    "      def call(self, inputs):\n",
    "            # inputs is a tensor of size B X N_id X D\n",
    "            N_id=tf.shape(inputs)[1]\n",
    "            return inputs + self._pos_encod()[:,:N_id,:]\n",
    "      \n",
    "class pos_encod_layer(tf.keras.layers.Layer):\n",
    "      def __init__(self, N_id_max, embedding_dim):\n",
    "            assert embedding_dim % 2 == 0, \"embedding_dim must be even\"\n",
    "            super(pos_encod_layer, self).__init__()\n",
    "\n",
    "            self.N_id_max=N_id_max\n",
    "            self.embedding_dim=embedding_dim\n",
    "            \n",
    "            # for trainable positional encoding, we want to make sure weights can be called \n",
    "            self.pos_encoding=self.add_weight(name=\"pos_encoding\",\n",
    "                                              shape=(1,N_id_max,embedding_dim),\n",
    "                                              initializer='random_normal',\n",
    "                                              trainable=True)\n",
    "            \n",
    "      def call(self,inputs):\n",
    "                  N_id=tf.shape(inputs)[1]\n",
    "                  return inputs + tf.expand_dims(self.pos_encoding[:N_id,:],0) \n",
    "                  # or self.pos_encoding[tf.newaxis,:N_id,:]\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2.2 Attention and Multi-Head Attention\n",
    "\n",
    "Now it comes to the main topic, the Multi-Head Attention layer. To understand what's going on, it's better to start with single-head attention. Here the math was done for encoder model. I will talk about the difference with other models later. From our pre-processing steps, we have the input as $B \\times N_{id} \\times d_{model}$, where $B$ is the batch size, $N_{id}$ is the input sequence length, and $d_{model}$ is the embedding dimension. The MHA process can be broadcast in the Batch size dimention. Let's set batch size to 1. Let's denote the data as the tensor $M_{ij}$, where $i=0,1,2,3,...,N_{id}-1$, and $j=0,1,2,3,...,d_{model}$. MHA will create three projection tensor, of the size $d_{model} \\times d_{model}$ ,$W^q,W^k,W^v$ (I put the lable in the superscript because I want to consistently put indices as subscript). Q stands for Query, K stands for Key, and V stands for value. We act these projection on the original input $M$ to obtain three tensor of size $N_{id} \\times d_{model}$,\n",
    "\\begin{equation}\n",
    "\\begin{split}\\notag\n",
    "Q_{ij}&=M_{il} W^q_{lj}\\\\\n",
    "K_{ij}&=M_{il} W^k_{lj} \\\\\n",
    "V_{ij}&=M_{il} W^v_{lj}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "where repeated indices are assumed to be summed. $\\vec{A} \\cdot \\vec{B}\\equiv A^iB^i$. We then procede to calculate the attention score, the overlap between the Query and the key. I will try my best to give intuiation about this later but let me lay out exactly the mathematical operation that happens in this step. The attention score is denoted as S, of size $N_{id} \\times N_{id}$\n",
    "\\begin{equation}\n",
    "S_{ij}=Q \\cdot K^T/\\sqrt{d_{model}}=Q_{il} (K^T)_{lj}/\\sqrt{d_{model}}=Q_{il}K_{jl}/\\sqrt{d_{model}} \\notag\n",
    "\\end{equation}\n",
    "perhaps a better way to see what this is to write Q, K, V as a vector of vectors of size $d_{model}$. For example, $Q=[\\vec q_0,\\vec q_1,....,\\vec q_{N_{id}-1}]$. In this way, we can clearly see that,\n",
    "\\begin{align} \\notag\n",
    "S_{ij}=\\vec q_i \\cdot \\vec k_j^T /\\sqrt{d_{model}}\n",
    "\\end{align}\n",
    "now the overlap can not be directly used, so we need some kind of probability and the goal is to write something like,\n",
    "\\begin{align}\n",
    "\\notag\n",
    "\\vec o_i=P_{ij} \\vec v_j\n",
    "\\end{align}\n",
    "The reason I didn't apply the softmax right away is that I want to demonstrate it clearly along which dimension you apply the softmax. To produce output at position i, you need to apply the softmax along the last index. For each overlap, one generate a new value vector with the weight $P_{ij}$, where,\n",
    "\\begin{equation}\n",
    "P_{ij}=\\frac{e^{S_{ij}}}{\\sum_j e^{S_{ij}}} \\notag\n",
    "\\end{equation}\n",
    "This whole operation will be trivial if all tensor are fixed. Namely, only the optimal state has meaning. After training, and assuming perfect training, what would happen is that $Q$ is the optimal representation of the data, $K$ is the optimal organization of the data and $V$ is the efficient representation of the output data. To be more specific, positional encoding plus embedding might not be enough, espcially if you are using the fixed map as shown in our example, to represent the data in an efficient way. While $K$ has to learn to extract relevent feature of the input data. Splitting the above procedure into multiple heads is just to do this calculation independent in subspace, then put the predicted result back together. As a convention, one put final projection $W^o$ on top of concatenated result, which is to increase the expressiveness of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-head attention layer\n",
    "# input is tensor B X N_id X d_{model}\n",
    "# attention, vector of D dimension\n",
    "# W_q W_k W_v => Q, K, V, eg: Q= inputs \\cdot W_q\n",
    "# attention score Q \\dot K^T   \n",
    "# Q is the information that is in your input\n",
    "# K is the information that you are comparing to\n",
    "# D= d_head * Nof_heads\n",
    "# 1000= 10 * 100\n",
    "class MHA(tf.keras.layers.Layer):\n",
    "        def __init__(self, embedding_dim, Nof_heads):\n",
    "            super(MHA,self).__init__()\n",
    "            assert embedding_dim % Nof_heads == 0, \"embedding_dim must be divisible by num_heads\"\n",
    "\n",
    "            self.embedding_dim=embedding_dim\n",
    "            self.Nof_heads=Nof_heads  \n",
    "            self.key_dim= embedding_dim // Nof_heads\n",
    " \n",
    "            # weigth matrices for Q,K,V\n",
    "            self.W_q=self.add_weight(name=\"W_q\",\n",
    "                                     shape=(embedding_dim, embedding_dim),\n",
    "                                     initializer=\"random_normal\",\n",
    "                                     trainable=True)\n",
    "            \n",
    "            self.W_k=self.add_weight(name=\"W_k\",\n",
    "                                     shape=(embedding_dim, embedding_dim),\n",
    "                                     initializer=\"random_normal\",\n",
    "                                     trainable=True)\n",
    "            \n",
    "            self.W_v=self.add_weight(name=\"W_v\",\n",
    "                                     shape=(embedding_dim, embedding_dim),\n",
    "                                     initializer=\"random_normal\",\n",
    "                                     trainable=True)\n",
    "            \n",
    "            self.W_out=self.add_weight(name=\"W_out\",\n",
    "                                     shape=(embedding_dim, embedding_dim),\n",
    "                                     initializer=\"random_normal\",\n",
    "                                     trainable=True)\n",
    "            \n",
    "        def split_heads(self,Vector):\n",
    "            \n",
    "            # input vector is a tensor of size B X N_id X D\n",
    "            # name vector emphasizes that it is a vector in the embedding space that we are splitting\n",
    "            # split the last dimension into (Nof_heads, key_dim)\n",
    "            # Output should be (B, Nof_heads, N_id, key_dim)\n",
    "\n",
    "\n",
    "            # Split the last dimension into (Nof_heads, key_dim)\n",
    "            input_reshaped=tf.reshape(Vector,(tf.shape(Vector)[0],tf.shape(Vector)[1],self.Nof_heads,self.key_dim))\n",
    "            # Transpose to get the standard format \n",
    "            \n",
    "            return tf.transpose(input_reshaped,perm=[0,2,1,3]) # B X Nof_heads X N_id  X key_dim\n",
    "        \n",
    "        def call(self,Q,K,V,mask=None):\n",
    "                \n",
    "                # Project Q,K,V\n",
    "                Q=tf.matmul(Q,self.W_q) # B X N_id X D\n",
    "                K=tf.matmul(K,self.W_k) # B X N_id X D\n",
    "                V=tf.matmul(V,self.W_v) # B X N_id X D\n",
    "                \n",
    "                # split the vector into different heads B X Nof_heads X N_id  X key_dim\n",
    "                Q=self.split_heads(Q)\n",
    "                K=self.split_heads(K)\n",
    "                V=self.split_heads(V)\n",
    "\n",
    "                # calculate the attention scores for each heads\n",
    "                # keep in mind that we have to use a tf tensor for the denominator \n",
    "                Scores= tf.matmul(Q,K,transpose_b=True) /tf.math.sqrt(tf.cast(self.key_dim,tf.float32))\n",
    "                # at each batch, head, Q=size of N_id X key_dim, K^T=size of key_dim X N_id \n",
    "                # Scores is a tensor of size B X Nof_heads X N_id X N_id\n",
    "                if mask is not None:\n",
    "                      \n",
    "                      mask=tf.where(mask == 0, -1e9, 0.0)\n",
    "                      Scores= Scores + mask\n",
    "\n",
    "                Soft_max=tf.nn.softmax(Scores,axis=-1)\n",
    "                 \n",
    "                # output per head is B X Nof_heads X N_id X key_dim\n",
    "                Output= tf.matmul(Soft_max,V) \n",
    "                # transpose back to the original format\n",
    "                Output=tf.transpose(Output,perm=[0,2,1,3])\n",
    "                # concatenate the heads\n",
    "                Output=tf.reshape(Output,(tf.shape(Q)[0], tf.shape(Q)[2], self.embedding_dim))\n",
    "                # alternatively but less explicitly one can do tf.reshape(Output,tf.shape(inputs))\n",
    "                # final projection\n",
    "                O=tf.matmul(Output, self.W_out)      \n",
    "               \n",
    "                return O \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have noticed that we have added mask to the attention scores. What this does is to remove the contribution of certain positions. The way we remove them is to add a huge negative number $-10^{9}$ to the attention score because $e^{-10^9} \\sim 0$. This will be sufficient to suppress the corresponding contributions. One type of mask is the padding mask, which record whether real information or padded zeros are at certain position. Another mask is the casual mask, which remove the effects of later token on the previous token. We will talk about them further after we have the code for encoder layer and decoder layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To increase the expressiveness of the model, we add the feed forward layer which has two dense layers where you first take the the output to higher dimension and reduce it back. Usually one take the higher dimension to be some integer times the embedding dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed forward network layer\n",
    "class Feed_forward_network(tf.keras.layers.Layer):\n",
    "        def __init__(self,embedding_dim, expanding_dim):\n",
    "            super(Feed_forward_network,self).__init__()\n",
    "            self.embedding_dim=embedding_dim\n",
    "            self.expanding_dim=expanding_dim\n",
    "            self.ffn=tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(expanding_dim,activation='relu'),\n",
    "                tf.keras.layers.Dense(embedding_dim)])\n",
    "        \n",
    "        def call(self,input):\n",
    "              return self.ffn(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2.3 Encoder and Decoder\n",
    "\n",
    "The transformer consists of an encoder and decoder which are simply stacking multiple encoder layers and decoder layers together. The encoder layer is: MHA layer-> layer normalization-> feed forward layer -> layer normalization. We only need to use the padding mask or source mask. Query, Key, and Value are all calculated from the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder_layer(tf.keras.layers.Layer):\n",
    "        # the input is result of positional encoding namely B X N_id X D\n",
    "        # in parameter needs to specify MHA and FFN\n",
    "        def __init__(self,embedding_dim,Nof_heads,expanding_dim):\n",
    "            super(encoder_layer,self).__init__()\n",
    "            #initialize the paramters\n",
    "            self.embedding_dim=embedding_dim\n",
    "            self.Nof_heads=Nof_heads\n",
    "            self.expanding_dim=expanding_dim\n",
    "            # create the layers\n",
    "            self.MHA=MHA(embedding_dim,Nof_heads,mask=None)\n",
    "            self.FFN=Feed_forward_network(embedding_dim,expanding_dim)\n",
    "            self.LN1=tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.LN2=tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        def call(self,inputs,source_mask=None):\n",
    "              # MHA\n",
    "              Out_MHA=self.MHA(inputs,inputs,inputs,mask=source_mask)\n",
    "              # residual connection+ layer normalization\n",
    "              Out1=self.LN1(inputs+Out_MHA)       \n",
    "              # Feed forward network\n",
    "              Out_ffn=self.FFN(Out1)\n",
    "              # residual connection+ layer normalization\n",
    "              return self.LN2(Out1+Out_ffn)     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, the decoder layer is slightly more flexible. It has one self-attention layer, one cross attention layer. For the self-attention layer, we always apply the causal mask (target mask)on the target data. While for the cross attention layer, you will compute Q from result of the first self-attention layer which acted on the target sequence. As for K and V, you will use the output from the encoder if it's a sequence to sequence model. In the case of decoder only model, there is no need for cross attention and one use target data for K and V as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class decoder_layer(tf.keras.layers.Layer):\n",
    "      def __init__(self, embedding_dim, Nof_heads,expanding_dim):\n",
    "            super(decoder_layer,self).__init__()\n",
    "            # initialize the paramters \n",
    "            self.embedding_dim=embedding_dim\n",
    "            self.Nof_heads=Nof_heads\n",
    "            self.expanding_dim=expanding_dim\n",
    "\n",
    "            # create the layers\n",
    "            self.MHA_masked=MHA(embedding_dim,Nof_heads, mask=None)   \n",
    "            self.MHA_cross= MHA(embedding_dim,Nof_heads, mask=None)   \n",
    "            self.FFN=Feed_forward_network(embedding_dim,expanding_dim)\n",
    "            self.LN1=tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.LN2=tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.LN3=tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "      def call(self,target,encoder_output,source_mask=None,target_mask=None):\n",
    "            #inputs could be encoder output or directly from the positional encoding\n",
    "            # MHA masked\n",
    "            Out_MHA_masked=self.MHA_masked(target,target,target,mask=target_mask)\n",
    "            # residual connection+ layer normalization\n",
    "            Out1=self.LN1(target+Out_MHA_masked)\n",
    "            # MHA cross\n",
    "            Out_MHA_cross=self.MHA_cross(Out1,encoder_output,encoder_output,mask=source_mask)\n",
    "            # residual connection+ layer normalization\n",
    "            Out2=self.LN2(Out1+Out_MHA_cross)\n",
    "            # Feed forward network\n",
    "            Out_ffn=self.FFN(Out2)\n",
    "            # residual connection+ layer normalization\n",
    "            return self.LN3(Out2+Out_ffn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "\n",
    "      def __init__(self, embedding_dim, Nof_heads,expanding_dim, N_layers):\n",
    "            super(Encoder,self).__init__()\n",
    "            #initialize the parameters\n",
    "            self.embedding_dim=embedding_dim\n",
    "            self.Nof_heads=Nof_heads\n",
    "            self.expanding_dim=expanding_dim\n",
    "            self.N_layers=N_layers\n",
    "            # create the layers\n",
    "            self.encoder_layers=[encoder_layer(embedding_dim,Nof_heads,expanding_dim) for _ in range(Nof_layers)]\n",
    "            \n",
    "      def call(self,inputs,source_mask=None):\n",
    "            #inputs is the output of the positional encoding\n",
    "            for i in range(self.N_layers):\n",
    "                  inputs=self.encoder_layers[i](inputs,source_mask)\n",
    "            return inputs\n",
    "      \n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "      def __init__(self, embedding_dim, Nof_heads, expanding_dim, N_layers):\n",
    "            super(Decoder,self).__init__()\n",
    "            #initialize the parameters\n",
    "            self.embedding_dim=embedding_dim\n",
    "            self.Nof_heads=Nof_heads\n",
    "            self.expanding_dim=expanding_dim\n",
    "            self.N_layers=N_layers\n",
    "            # create the layers\n",
    "            self.decoder_layers=[decoder_layer(embedding_dim,Nof_heads,expanding_dim) for _ in range(N_layers)]\n",
    "            \n",
    "      def call(self,target,encoder_output,source_mask=None,target_mask=None):\n",
    "            #inputs is the output of the positional encoding\n",
    "            for i in range(self.N_layers):\n",
    "                  target=self.decoder_layers[i](target,encoder_output,source_mask,target_mask)\n",
    "            return target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have included the most general architecture, without details like dropout layer. In the next post, we will write code for the decoder, encoder and sequence-to-sequence and demonstrate how inference works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
