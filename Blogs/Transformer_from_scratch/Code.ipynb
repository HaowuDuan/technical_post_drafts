{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by writing multi-head attention class in tensorflow. To do this, we assume our input is already tokenized vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will import a pre-trained tokenizer, that will map text into list of token ID. And this is just list of numbers. The tokenizer we used has sub-word tokenization, but it is useful to think that we have one ID for each individual word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[  101  7592  1010  2026  3899  2003 10140   102     0     0]], shape=(1, 10), dtype=int32)\n",
      "tf.Tensor([[1 1 1 1 1 1 1 1 0 0]], shape=(1, 10), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer,TFBertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenization_test(input_test):\n",
    "    inputs = tokenizer(input_test, return_tensors='tf',max_length=10,return_attention_mask=True,padding='max_length')\n",
    "    ID_test=inputs['input_ids']\n",
    "    mask=inputs[\"attention_mask\"]\n",
    "    print(ID_test)\n",
    "    print(mask)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tokenization_test(\"Hello, my dog is cute\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code snipt is just to get a sense of how things works. If we use BERT tokenizer, for each batch, the maximal allowed sequence length is 512. One can choose truncation or simply returning an error by setting truncation=true or faule in the tokenizer arugment. The next step will be embeding and positional ecoding. There are multiple ways to do it. In my small llm code, I will use pre-defined model since the focus is the transformer architecture. But it's also good to understand how this step works, both in practise and from the original paper \"Attention is all you need\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we preparing out training data, we will add padding into tokenizer because the training can only handle batches with the same sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_shape=(1, 6, 768)\n"
     ]
    }
   ],
   "source": [
    "# Here we work with single batch, and let the length of token ID to be N_id\n",
    "# The input here is 1 X N_id tensor\n",
    "# The embedding is the process where you map the token ID at each position to a vector of size D\n",
    "# the positional encoding is creatig another vector of the same size D\n",
    "# Model output is a tensor of size 1 X N_id X D\n",
    "\n",
    "Embedding_model=TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def embedding_test(input_test):\n",
    "    inputs = tokenizer(input_test, return_tensors='tf')\n",
    "    Embedding_tmp=Embedding_model(inputs)\n",
    "    output_embedding=Embedding_tmp.last_hidden_state\n",
    "\n",
    "    print(f\"output_shape={output_embedding.shape}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    embedding_test(\"Make America Great Again\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow offers embedding layer for custom embedding\n",
    "# embedding = tf.keras.layers.Embedding(input_dim=N_id, output_dim=embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixed positional encoding is included for demonstration purpose\n",
    "# In practice, we can use the positional encoding layer that can be optimized during training\n",
    "# Here we assumed that N_id is N_id_max\n",
    "# Tokenization will called the number N_id for padding\n",
    "# later we will add attention mask to deal with this during calculation of attention scores\n",
    "class fixed_pos_encod_layer(tf.keras.layers.Layer):\n",
    "      def __init__(self, N_id_max, embedding_dim):\n",
    "            assert embedding_dim % 2 == 0, \"embedding_dim must be even\"\n",
    "            super(fixed_pos_encod_layer, self).__init__()\n",
    "\n",
    "            self.N_id_max=N_id_max\n",
    "            self.embedding_dim=embedding_dim\n",
    "    \n",
    "      def _pos_encod(self):\n",
    "            # position range from 0 to N_id-1\n",
    "            # i range from 0 to embedding_dim-1\n",
    "            # Due to the alternating pattern of sin and cos, by convention, embedding_dim is even\n",
    "            # Position indices\n",
    "            pos_index = tf.range(self.N_id_max, dtype=tf.float32)[:, tf.newaxis]\n",
    "            # frequency factors, note that it is more efficient to use tf.exp then tf.pow\n",
    "            omega = tf.exp(-2*tf.range(0,self.embedding_dim,2,dtype=tf.float32)/self.embedding_dim *tf.math.log(10000.0))\n",
    "            \n",
    "            angles=pos_index*omega \n",
    "\n",
    "            pos_encoding=tf.concat([tf.sin(angles),tf.cos(angles)],axis=-1)\n",
    "            \n",
    "            # add batch dimension\n",
    "            # prepare for broadcasting\n",
    "            return tf.expand_dims(pos_encoding,0)\n",
    "      \n",
    "      def call(self, inputs):\n",
    "            # inputs is a tensor of size B X N_id X D\n",
    "            N_id=tf.shape(inputs)[1]\n",
    "            return inputs + self._pos_encod()[:,:N_id,:]\n",
    "      \n",
    "class pos_encod_layer(tf.keras.layers.Layer):\n",
    "      def __init__(self, N_id_max, embedding_dim):\n",
    "            assert embedding_dim % 2 == 0, \"embedding_dim must be even\"\n",
    "            super(pos_encod_layer, self).__init__()\n",
    "\n",
    "            self.N_id_max=N_id_max\n",
    "            self.embedding_dim=embedding_dim\n",
    "            \n",
    "            # for trainable positional encoding, we want to make sure weights can be called \n",
    "            self.pos_encoding=self.add_weight(name=\"pos_encoding\",\n",
    "                                              shape=(1,N_id_max,embedding_dim),\n",
    "                                              initializer='random_normal',\n",
    "                                              trainable=True)\n",
    "            \n",
    "      def call(self,inputs):\n",
    "                  N_id=tf.shape(inputs)[1]\n",
    "                  return inputs + tf.expand_dims(self.pos_encoding[:N_id,:],0) \n",
    "                  # or self.pos_encoding[tf.newaxis,:N_id,:]\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our data ready to feed into transformer. A transformer is consist of encoder or decoder or both. Their underling structure is multi-headed attention, which is the core idea behind \"attention is all you need\". In a Transformer, the conventional architecture uses one Self-MHA followed by an FFN per encoder layer to process the input sequence, and two attention mechanisms (Self-MHA then regular MHA) followed by an FFN per decoder layer to handle the target sequence and connect to the encoderâ€™s output, with multiple such layers stacked in both the encoder and decoder to form the full model, though you can tweak this setup for custom experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-head attention layer\n",
    "# input is tensor B X N_id X D \n",
    "# attention, vector of D dimension\n",
    "# W_q W_k W_v => Q, K, V, eg: Q= inputs \\cdot W_q\n",
    "# attention score Q \\dot K^T   \n",
    "# Q is the information that is in your input\n",
    "# K is the information that you are comparing to\n",
    "# D= d_head * Nof_heads\n",
    "# 1000= 10 * 100\n",
    "class MHA(tf.keras.layers.Layer):\n",
    "        def __init__(self, embedding_dim, Nof_heads):\n",
    "            super(MHA,self).__init__()\n",
    "            assert embedding_dim % Nof_heads == 0, \"embedding_dim must be divisible by num_heads\"\n",
    "\n",
    "            self.embedding_dim=embedding_dim\n",
    "            self.Nof_heads=Nof_heads  \n",
    "            self.key_dim= embedding_dim // Nof_heads\n",
    " \n",
    "            # weigth matrices for Q,K,V\n",
    "            self.W_q=self.add_weight(name=\"W_q\",\n",
    "                                     shape=(embedding_dim, embedding_dim),\n",
    "                                     initializer=\"random_normal\",\n",
    "                                     trainable=True)\n",
    "            \n",
    "            self.W_k=self.add_weight(name=\"W_k\",\n",
    "                                     shape=(embedding_dim, embedding_dim),\n",
    "                                     initializer=\"random_normal\",\n",
    "                                     trainable=True)\n",
    "            \n",
    "            self.W_v=self.add_weight(name=\"W_v\",\n",
    "                                     shape=(embedding_dim, embedding_dim),\n",
    "                                     initializer=\"random_normal\",\n",
    "                                     trainable=True)\n",
    "            \n",
    "            self.W_out=self.add_weight(name=\"W_out\",\n",
    "                                     shape=(embedding_dim, embedding_dim),\n",
    "                                     initializer=\"random_normal\",\n",
    "                                     trainable=True)\n",
    "            \n",
    "        def split_heads(self,Vector):\n",
    "            \n",
    "            # input vector is a tensor of size B X N_id X D\n",
    "            # name vector emphasizes that it is a vector in the embedding space that we are splitting\n",
    "            # split the last dimension into (Nof_heads, key_dim)\n",
    "            # Output should be (B, Nof_heads, N_id, key_dim)\n",
    "\n",
    "\n",
    "            # Split the last dimension into (Nof_heads, key_dim)\n",
    "            input_reshaped=tf.reshape(Vector,(tf.shape(Vector)[0],tf.shape(Vector)[1],self.Nof_heads,self.key_dim))\n",
    "            # Transpose to get the standard format \n",
    "            \n",
    "            return tf.transpose(input_reshaped,perm=[0,2,1,3]) # B X Nof_heads X N_id  X key_dim\n",
    "        \n",
    "        def call(self,Q,K,V,mask=None):\n",
    "                \n",
    "                # Project Q,K,V\n",
    "                Q=tf.matmul(Q,self.W_q) # B X N_id X D\n",
    "                K=tf.matmul(K,self.W_k) # B X N_id X D\n",
    "                V=tf.matmul(V,self.W_v) # B X N_id X D\n",
    "                \n",
    "                # split the vector into different heads B X Nof_heads X N_id  X key_dim\n",
    "                Q=self.split_heads(Q)\n",
    "                K=self.split_heads(K)\n",
    "                V=self.split_heads(V)\n",
    "\n",
    "                # calculate the attention scores for each heads\n",
    "                # keep in mind that we have to use a tf tensor for the denominator \n",
    "                Scores= tf.matmul(Q,K,transpose_b=True) /tf.math.sqrt(tf.cast(self.key_dim,tf.float32))\n",
    "                # at each batch, head, Q=size of N_id X key_dim, K^T=size of key_dim X N_id \n",
    "                # Scores is a tensor of size B X Nof_heads X N_id X N_id\n",
    "                if mask is not None:\n",
    "                      \n",
    "                      mask=tf.where(mask == 0, -1e9, 0.0)\n",
    "                      Scores= Scores + mask\n",
    "\n",
    "                Soft_max=tf.nn.softmax(Scores,axis=-1)\n",
    "                 \n",
    "                # output per head is B X Nof_heads X N_id X key_dim\n",
    "                Output= tf.matmul(Soft_max,V) \n",
    "                # transpose back to the original format\n",
    "                Output=tf.transpose(Output,perm=[0,2,1,3])\n",
    "                # concatenate the heads\n",
    "                Output=tf.reshape(Output,(tf.shape(Q)[0], tf.shape(Q)[2], self.embedding_dim))\n",
    "                # alternatively but less explicitly one can do tf.reshape(Output,tf.shape(inputs))\n",
    "                # final projection\n",
    "                O=tf.matmul(Output, self.W_out)      \n",
    "               \n",
    "                return O \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed forward network layer\n",
    "class Feed_forward_network(tf.keras.layers.Layer):\n",
    "        def __init__(self,embedding_dim, expanding_dim):\n",
    "            super(Feed_forward_network,self).__init__()\n",
    "            self.embedding_dim=embedding_dim\n",
    "            self.expanding_dim=expanding_dim\n",
    "            self.ffn=tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(expanding_dim,activation='relu'),\n",
    "                tf.keras.layers.Dense(embedding_dim)])\n",
    "        \n",
    "        def call(self,input):\n",
    "              return self.ffn(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder_layer(tf.keras.layers.Layer):\n",
    "        # the input is result of positional encoding namely B X N_id X D\n",
    "        # in parameter needs to specify MHA and FFN\n",
    "        def __init__(self,embedding_dim,Nof_heads,expanding_dim):\n",
    "            super(encoder_layer,self).__init__()\n",
    "            #initialize the paramters\n",
    "            self.embedding_dim=embedding_dim\n",
    "            self.Nof_heads=Nof_heads\n",
    "            self.expanding_dim=expanding_dim\n",
    "            # create the layers\n",
    "            self.MHA=MHA(embedding_dim,Nof_heads,mask=None)\n",
    "            self.FFN=Feed_forward_network(embedding_dim,expanding_dim)\n",
    "            self.LN1=tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.LN2=tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        def call(self,inputs,source_mask=None):\n",
    "              # MHA\n",
    "              Out_MHA=self.MHA(inputs,inputs,inputs,mask=source_mask)\n",
    "              # residual connection+ layer normalization\n",
    "              Out1=self.LN1(inputs+Out_MHA)       \n",
    "              # Feed forward network\n",
    "              Out_ffn=self.FFN(Out1)\n",
    "              # residual connection+ layer normalization\n",
    "              return self.LN2(Out1+Out_ffn)     \n",
    "\n",
    "class decoder_layer(tf.keras.layers.Layer):\n",
    "      def __init__(self, embedding_dim, Nof_heads,expanding_dim):\n",
    "            super(decoder_layer,self).__init__()\n",
    "            # initialize the paramters \n",
    "            self.embedding_dim=embedding_dim\n",
    "            self.Nof_heads=Nof_heads\n",
    "            self.expanding_dim=expanding_dim\n",
    "\n",
    "            # create the layers\n",
    "            self.MHA_masked=MHA(embedding_dim,Nof_heads, mask=None)   \n",
    "            self.MHA_cross= MHA(embedding_dim,Nof_heads, mask=None)   \n",
    "            self.FFN=Feed_forward_network(embedding_dim,expanding_dim)\n",
    "            self.LN1=tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.LN2=tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.LN3=tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "      def call(self,target,encoder_output,source_mask=None,target_mask=None):\n",
    "            #inputs could be encoder output or directly from the positional encoding\n",
    "            # MHA masked\n",
    "            Out_MHA_masked=self.MHA_masked(target,target,target,mask=target_mask)\n",
    "            # residual connection+ layer normalization\n",
    "            Out1=self.LN1(target+Out_MHA_masked)\n",
    "            # MHA cross\n",
    "            Out_MHA_cross=self.MHA_cross(Out1,encoder_output,encoder_output,mask=source_mask)\n",
    "            # residual connection+ layer normalization\n",
    "            Out2=self.LN2(Out1+Out_MHA_cross)\n",
    "            # Feed forward network\n",
    "            Out_ffn=self.FFN(Out2)\n",
    "            # residual connection+ layer normalization\n",
    "            return self.LN3(Out2+Out_ffn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do decoder layer and decoder, in which we simply loop through the layers from a list and pass through the result to the next layer.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "\n",
    "      # \n",
    "      def __init__(self, embedding_dim, Nof_heads,expanding_dim, N_layers):\n",
    "            super(Encoder,self).__init__()\n",
    "            #initialize the parameters\n",
    "            self.embedding_dim=embedding_dim\n",
    "            self.Nof_heads=Nof_heads\n",
    "            self.expanding_dim=expanding_dim\n",
    "            self.N_layers=N_layers\n",
    "            # create the layers\n",
    "            self.encoder_layers=[encoder_layer(embedding_dim,Nof_heads,expanding_dim) for _ in range(Nof_layers)]\n",
    "            \n",
    "      def call(self,inputs,source_mask=None):\n",
    "            #inputs is the output of the positional encoding\n",
    "            for i in range(self.N_layers):\n",
    "                  inputs=self.encoder_layers[i](inputs,source_mask)\n",
    "            return inputs\n",
    "      \n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "      def __init__(self, embedding_dim, Nof_heads, expanding_dim, N_layers):\n",
    "            super(Decoder,self).__init__()\n",
    "            #initialize the parameters\n",
    "            self.embedding_dim=embedding_dim\n",
    "            self.Nof_heads=Nof_heads\n",
    "            self.expanding_dim=expanding_dim\n",
    "            self.N_layers=N_layers\n",
    "            # create the layers\n",
    "            self.decoder_layers=[decoder_layer(embedding_dim,Nof_heads,expanding_dim) for _ in range(N_layers)]\n",
    "            \n",
    "      def call(self,target,encoder_output,source_mask=None,target_mask=None):\n",
    "            #inputs is the output of the positional encoding\n",
    "            for i in range(self.N_layers):\n",
    "                  target=self.decoder_layers[i](target,encoder_output,source_mask,target_mask)\n",
    "            return target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For text summerization, we need both the encoder and decoder. Which has the following attributes: embedding_dim, Nof_heads, expanding_factor, N_layer. The following the part 2 of the transformer blog post. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 4358\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 36718\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3760\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset=load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "# we first understand the structure of the dataset \n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that train_split has given us three subset. For our purpose, just the train part will be sufficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      " = Valkyria Chronicles III = \n",
      "\n",
      "{'text': ' = Valkyria Chronicles III = \\n'}\n"
     ]
    }
   ],
   "source": [
    "training_data=dataset['train']['text']\n",
    "print(type(training_data))\n",
    "print(training_data[1])\n",
    "print(dataset['train'][1])\n",
    "# df = pd.DataFrame(training_data, columns=[\"text\"])\n",
    "# print(\"DataFrame Info:\")\n",
    "# print(df.info())\n",
    "# print(\"\\nFirst 5 rows:\")\n",
    "# print(df.head())\n",
    "# print(\"\\nBasic stats:\")\n",
    "# print(df.describe())\n",
    "# print(\"\\nSample 5 random rows:\")\n",
    "# print(df.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[ 1027 11748  4801  4360 11906  3523  1027     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0], shape=(50,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# tokenize data\n",
    "\n",
    "Tokenized_data=tokenizer(training_data,\n",
    "                          return_tensors='tf',\n",
    "                          padding=\"max_length\",\n",
    "                          truncation=True,\n",
    "                          add_special_tokens=False,\n",
    "                          max_length=50,\n",
    "                          return_attention_mask=True)\n",
    "\n",
    "input_ids=Tokenized_data['input_ids']\n",
    "padding_mask=Tokenized_data['attention_mask']\n",
    "\n",
    "print(input_ids[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[ 1996  2208  1005  1055  2645  2291  1010  1996 22312  2291  1010  2003\n",
      "  3344  2058  3495  2013 11748  4801  7895 11906  1012  2076  6416  1010\n",
      "  2867  7276  2169  3131  2478  1037  2327  1030  1011  1030  2091  7339\n",
      "  1997  1996 11686  4949  1024  2320  1037  2839  2003  3479  1010  1996\n",
      "  2447  5829], shape=(50,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(input_ids[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take tokenized data and give us batched training and target sequences, and causal mask \n",
    "class token_to_data():\n",
    "        def __init__(self,input_ids,padding_mask,batch_size):\n",
    "               self.input_ids=input_ids\n",
    "               self.padding_mask=tf.identity(padding_mask)\n",
    "               self.batch_size=batch_size\n",
    "               self._data_pre()\n",
    "               self._create_dataset()\n",
    "\n",
    "        def _data_pre(self):\n",
    "                #sequence length\n",
    "                self.seq_len=tf.shape(self.input_ids)[-1]-1\n",
    "\n",
    "                # parameters for batches\n",
    "                N_seq  =  tf.shape(self.input_ids)[0]\n",
    "                self.N_batch=  N_seq // self.batch_size\n",
    "                N_trimmed=  self.N_batch * self.batch_size\n",
    "\n",
    "                # input sequence\n",
    "                self.input_sequence=self.input_ids[:N_trimmed,:-1]\n",
    "                # target sequence\n",
    "                self.target_sequence=self.input_ids[:N_trimmed,1:] \n",
    "                # padding mask\n",
    "                self.padding_mask=self.padding_mask[:N_trimmed,:-1]\n",
    "                \n",
    "                # batch the data\n",
    "                self.batched_input_sequence=tf.reshape(self.input_sequence,(self.N_batch,self.batch_size,self.seq_len))\n",
    "                self.batched_target_sequence=tf.reshape(self.target_sequence,(self.N_batch,self.batch_size,self.seq_len))\n",
    "                self.batched_padding_mask=tf.reshape(self.padding_mask,(self.N_batch,self.batch_size,1,1,self.seq_len))\n",
    "\n",
    "                # compute causal mask\n",
    "                self.causal_mask=tf.linalg.band_part(tf.ones((self.seq_len, self.seq_len)), -1, 0)\n",
    "                self.causal_mask=tf.reshape(self.causal_mask,(1,1,self.seq_len, self.seq_len))\n",
    "        \n",
    "        \n",
    "        def _create_dataset(self):\n",
    "                self.dataset = tf.data.Dataset.from_tensor_slices((self.batched_input_sequence, self.batched_target_sequence, self.batched_padding_mask) )\n",
    "                self.dataset = self.dataset.shuffle(buffer_size=1000).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        def get_dataset(self):\n",
    "            return self.dataset\n",
    "\n",
    "        def get_batch(self, batch_id):\n",
    "                return (self.batched_input_sequence[batch_id],\n",
    "                                        self.batched_target_sequence[batch_id],\n",
    "                                          self.batched_padding_mask[batch_id])\n",
    "        def get_causal_mask(self):\n",
    "               return self.causal_mask\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class decoderonlylayer(tf.keras.layers.Layer):\n",
    "      def __init__(self, embedding_dim, Nof_heads,expanding_dim):\n",
    "            super(decoderonlylayer,self).__init__()\n",
    "            # initialize the paramters \n",
    "            self.embedding_dim=embedding_dim\n",
    "            self.Nof_heads=Nof_heads\n",
    "            self.expanding_dim=expanding_dim\n",
    "\n",
    "            # create the layers\n",
    "            self.MHA_masked=MHA(embedding_dim,Nof_heads)   \n",
    "            self.FFN=Feed_forward_network(embedding_dim,expanding_dim)\n",
    "            self.LN1=tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.LN2=tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "      \n",
    "\n",
    "      def call(self,input,mask=None):\n",
    "            #inputs could be encoder output or directly from the positional encoding\n",
    "            # MHA masked\n",
    "            Out_MHA_masked=self.MHA_masked(input,input,input,mask=mask)\n",
    "            # residual connection+ layer normalization\n",
    "            Out1=self.LN1(input+Out_MHA_masked)\n",
    "            # residual connection+ layer normalization\n",
    "            # Feed forward network\n",
    "            Out_ffn=self.FFN(Out1)\n",
    "            # residual connection+ layer normalization\n",
    "            return self.LN2(Out1+Out_ffn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we simplify things and stacking decoder layer directly\n",
    "class next_token_prediction(tf.keras.Model):\n",
    "      def __init__(self,vocab_size,N_id_max,embedding_dim,Nof_heads,expanding_dim,N_layers):\n",
    "            # embedding_dim is the size of the embedding vector, denoted as D\n",
    "            # Nof_heads is the number of heads in the multi-head attention layer\n",
    "            # expanding_factor is the factor by which the embedding vector is expanded in the feed forward network\n",
    "            # N_layers is the number of layers in the encoder and decoder\n",
    "            super(next_token_prediction,self).__init__()\n",
    "\n",
    "            self.embedding_layer = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "            self.position_encoding= fixed_pos_encod_layer(N_id_max,embedding_dim)\n",
    "            self.decoder_layers=[decoderonlylayer(embedding_dim,Nof_heads,expanding_dim) for _ in range(N_layers)]\n",
    "            self.output_layer = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "      def call(self,input,mask=None):\n",
    "            tmp=self.embedding_layer(input)\n",
    "            tmp=self.position_encoding(tmp)\n",
    "            for layer in self.decoder_layers:\n",
    "                  tmp=layer(tmp,mask=mask)\n",
    "            return self.output_layer(tmp)      \n",
    "                  \n",
    "\n",
    "model= next_token_prediction(vocab_size = 30522,\n",
    "                             N_id_max=50,\n",
    "                             embedding_dim = 128,\n",
    "                             Nof_heads = 4,\n",
    "                             expanding_dim = 512,\n",
    "                             N_layers = 2)           \n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.001)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids=Tokenized_data['input_ids']\n",
    "padding_mask=Tokenized_data['attention_mask']\n",
    "\n",
    "\n",
    "data = token_to_data(input_ids, padding_mask, batch_size=32)\n",
    "dataset = data.get_dataset()\n",
    "causal_mask = data.get_causal_mask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inputs, targets, causal_mask):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, mask=causal_mask)\n",
    "        loss = loss_fn(targets, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Average loss: 2.9560\n",
      "Epoch 2/5\n",
      "Average loss: 2.6811\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-01 15:56:35.594281: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: 2.4780\n",
      "Epoch 4/5\n",
      "Average loss: 2.2810\n",
      "Epoch 5/5\n",
      "Average loss: 2.0946\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    for inputs, targets, padding_mask in dataset:\n",
    "        loss = train_step(inputs, targets, causal_mask)\n",
    "        total_loss += loss\n",
    "        num_batches += 1\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"Average loss: {avg_loss:.4f}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Now let's do inference \n",
    "\n",
    "# def text_generator(input_string, model, max_length, tokenizer):\n",
    "#     input_tokens =tokenizer(input_string, add_special_tokens=False,return_tensors='tf')\n",
    "#     input_ids = input_tokens['input_ids']\n",
    "#     generated = input_tokens.copy()\n",
    "\n",
    "#     for _ in range(max_length - len(input_tokens)):\n",
    "#         seq_len = tf.shape(input_ids)[1]\n",
    "#         causal_mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)  # Shape: (seq_len, seq_len)\n",
    "#         causal_mask = tf.reshape(causal_mask, (1, 1, seq_len, seq_len))\n",
    "\n",
    "#         logits = model(input_ids, mask=causal_mask)\n",
    "#         next_token_logits = logits[:, -1, :]\n",
    "#         next_token = tf.argmax(next_token_logits, axis=-1, output_type=tf.int32)\n",
    "#         generated.append(next_token.numpy()[0])\n",
    "\n",
    "#         print(tokenizer.decode(generated))\n",
    "#         input_ids = tf.constant([generated], dtype=tf.int32)\n",
    "#         if next_token.numpy()[0] == 102:\n",
    "#             break\n",
    "    \n",
    "\n",
    "def text_generator(input_string, model, max_length, tokenizer):\n",
    "    # Tokenize input string and get token IDs as a list\n",
    "    input_tokens = tokenizer.encode(input_string, add_special_tokens=False)  # Returns a list\n",
    "    input_ids = tf.constant([input_tokens], dtype=tf.int32)  # Shape: (1, input_len)\n",
    "    generated = input_tokens.copy()  # Now a list\n",
    "\n",
    "    for _ in range(max_length - len(input_tokens)):  # len works on list\n",
    "        seq_len = tf.shape(input_ids)[1]\n",
    "        causal_mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        causal_mask = tf.reshape(causal_mask, (1, 1, seq_len, seq_len))\n",
    "\n",
    "        logits = model(input_ids, mask=causal_mask)\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        next_token_logits = tf.where(tf.range(tf.shape(next_token_logits)[-1]) == 0, -1e9, next_token_logits)\n",
    "        next_token = tf.argmax(next_token_logits, axis=-1, output_type=tf.int32)\n",
    "        generated.append(next_token.numpy()[0])  # Append to list\n",
    "        \n",
    "        # Overwrite previous output with new string\n",
    "        print(tokenizer.decode(generated), end='\\r', flush=True)\n",
    "        input_ids = tf.constant([generated], dtype=tf.int32)\n",
    "        if next_token.numpy()[0] == 102:  # Stop at [SEP]\n",
    "            break\n",
    "    \n",
    "    final_text = tokenizer.decode(generated)\n",
    "    print(final_text)  # Final print without \\r to keep it\n",
    "    return final_text   \n",
    "         \n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love dogs and chains ( welsh : ffordd g. 39 ( 3 ) ( piano ), rac ( welsh : ffordd gyswllt, and the most common name of the \" king's \" ( \" ).\n"
     ]
    }
   ],
   "source": [
    "input_str=\"I love dogs\"\n",
    "generated_text = text_generator(model=model,\n",
    "                 input_string=input_str,\n",
    "                  max_length=50,\n",
    "                  tokenizer=tokenizer\n",
    "                  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
